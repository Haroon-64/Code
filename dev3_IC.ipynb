{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "NUM_FILES = 100\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = (300,400)  # len, width\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_folder, num_files=None):\n",
    "    \"\"\"\n",
    "    Read Spectrograms data from .npy files in the specified data folder.\n",
    "\n",
    "    Parameters:\n",
    "    - data_folder (str): Path to the main data folder containing 'train' and 'test' subfolders.\n",
    "    - num_files (int or None): Number of files to read from each subfolder. If None, all files will be read.\n",
    "\n",
    "    Returns:\n",
    "    - train (array[Tuple[np.ndarray, np.ndarray]]): List of tuples containing train EEG data.\n",
    "    - test (array[Tuple[np.ndarray, np.ndarray]]): List of tuples containing test EEG data.\n",
    "    - train_labels (pd.DataFrame): DataFrame containing train labels.\n",
    "    - test_labels (pd.DataFrame): DataFrame containing test labels.\n",
    "    \"\"\"\n",
    "    train_spec_folder = os.path.join(data_folder, 'train_spectrograms')\n",
    "    test_spec_folder = os.path.join(data_folder, 'test_spectrograms')\n",
    "\n",
    "    def read_npy_folder(folder_path, n_files=None):\n",
    "        arrays = []\n",
    "        files_to_read = os.listdir(folder_path)[:n_files] if n_files else os.listdir(folder_path)\n",
    "        for file in files_to_read:\n",
    "            if file.endswith('.npy'):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                array = np.load(file_path)\n",
    "                arrays.append(array)\n",
    "        print(f\"Read {len(arrays)} files from {folder_path}.\")\n",
    "        return arrays\n",
    "\n",
    "    # Read EEG data\n",
    "    train_spec = read_npy_folder(train_spec_folder, num_files)\n",
    "    test_spec = read_npy_folder(test_spec_folder)\n",
    "\n",
    "    train_labels = pd.read_csv(os.path.join(data_folder, 'train.csv'), nrows=num_files)\n",
    "    test_labels = pd.read_csv(os.path.join(data_folder, 'test.csv'))\n",
    "\n",
    "    return train_spec, test_spec, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 100 files from data/npy_data/npy_data/train_spectrograms.\n",
      "Read 1 files from data/npy_data/npy_data/test_spectrograms.\n"
     ]
    }
   ],
   "source": [
    "train,_test,train_labels,_test_labels = read_data('data/npy_data/npy_data',num_files=NUM_FILES)\n",
    "labels = pd.read_csv('train.csv', nrows=NUM_FILES)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train, train_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_train = label_encoder.fit_transform(y_train[:, 8])\n",
    "encoded_labels_val = label_encoder.fit_transform(y_val[:, 8])\n",
    "\n",
    "y_train = torch.nn.functional.one_hot(torch.tensor(encoded_labels_train), num_classes=NUM_CLASSES).float()\n",
    "y_val = torch.nn.functional.one_hot(torch.tensor(encoded_labels_val), num_classes=NUM_CLASSES).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 401)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 75 * 100, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = Image.fromarray(image)\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  \n",
    "    transforms.ToTensor(),       \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = ImageDataset(X_val, y_val, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel(num_classes=NUM_CLASSES)\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Mini-batch 10] Loss: -53173.604\n",
      "[Epoch 2, Mini-batch 10] Loss: -425050.197\n",
      "[Epoch 3, Mini-batch 10] Loss: -1859996.387\n",
      "[Epoch 4, Mini-batch 10] Loss: -5770604.025\n",
      "[Epoch 5, Mini-batch 10] Loss: -14119159.000\n",
      "[Epoch 6, Mini-batch 10] Loss: -29182701.200\n",
      "[Epoch 7, Mini-batch 10] Loss: -54283955.600\n",
      "[Epoch 8, Mini-batch 10] Loss: -93831564.800\n",
      "[Epoch 9, Mini-batch 10] Loss: -149627143.200\n",
      "[Epoch 10, Mini-batch 10] Loss: -228330352.000\n",
      "[Epoch 11, Mini-batch 10] Loss: -332936244.800\n",
      "[Epoch 12, Mini-batch 10] Loss: -467473936.000\n",
      "[Epoch 13, Mini-batch 10] Loss: -640046035.200\n",
      "[Epoch 14, Mini-batch 10] Loss: -852147718.400\n",
      "[Epoch 15, Mini-batch 10] Loss: -1108689452.800\n",
      "[Epoch 16, Mini-batch 10] Loss: -1421107315.200\n",
      "[Epoch 17, Mini-batch 10] Loss: -1789671180.800\n",
      "[Epoch 18, Mini-batch 10] Loss: -2217991705.600\n",
      "[Epoch 19, Mini-batch 10] Loss: -2722741248.000\n",
      "[Epoch 20, Mini-batch 10] Loss: -3292950220.800\n",
      "[Epoch 21, Mini-batch 10] Loss: -3944579660.800\n",
      "[Epoch 22, Mini-batch 10] Loss: -4685051468.800\n",
      "[Epoch 23, Mini-batch 10] Loss: -5516732825.600\n",
      "[Epoch 24, Mini-batch 10] Loss: -6453270937.600\n",
      "[Epoch 25, Mini-batch 10] Loss: -7486293504.000\n",
      "[Epoch 26, Mini-batch 10] Loss: -8631218841.600\n",
      "[Epoch 27, Mini-batch 10] Loss: -9901639884.800\n",
      "[Epoch 28, Mini-batch 10] Loss: -11287390208.000\n",
      "[Epoch 29, Mini-batch 10] Loss: -12801848934.400\n",
      "[Epoch 30, Mini-batch 10] Loss: -14488272179.200\n",
      "[Epoch 31, Mini-batch 10] Loss: -16278075084.800\n",
      "[Epoch 32, Mini-batch 10] Loss: -18248842035.200\n",
      "[Epoch 33, Mini-batch 10] Loss: -20355103539.200\n",
      "[Epoch 34, Mini-batch 10] Loss: -22623634636.800\n",
      "[Epoch 35, Mini-batch 10] Loss: -25085417472.000\n",
      "[Epoch 36, Mini-batch 10] Loss: -27719701094.400\n",
      "[Epoch 37, Mini-batch 10] Loss: -30534975283.200\n",
      "[Epoch 38, Mini-batch 10] Loss: -33546706329.600\n",
      "[Epoch 39, Mini-batch 10] Loss: -36749383680.000\n",
      "[Epoch 40, Mini-batch 10] Loss: -40168836710.400\n",
      "[Epoch 41, Mini-batch 10] Loss: -43826160844.800\n",
      "[Epoch 42, Mini-batch 10] Loss: -47633452236.800\n",
      "[Epoch 43, Mini-batch 10] Loss: -51772906700.800\n",
      "[Epoch 44, Mini-batch 10] Loss: -56103967948.800\n",
      "[Epoch 45, Mini-batch 10] Loss: -60656228761.600\n",
      "[Epoch 46, Mini-batch 10] Loss: -65450204364.800\n",
      "[Epoch 47, Mini-batch 10] Loss: -70592992460.800\n",
      "[Epoch 48, Mini-batch 10] Loss: -75919336243.200\n",
      "[Epoch 49, Mini-batch 10] Loss: -81590006579.200\n",
      "[Epoch 50, Mini-batch 10] Loss: -87491223552.000\n",
      "[Epoch 51, Mini-batch 10] Loss: -93689358745.600\n",
      "[Epoch 52, Mini-batch 10] Loss: -100131719577.600\n",
      "[Epoch 53, Mini-batch 10] Loss: -106981353062.400\n",
      "[Epoch 54, Mini-batch 10] Loss: -114187221401.600\n",
      "[Epoch 55, Mini-batch 10] Loss: -121621436006.400\n",
      "[Epoch 56, Mini-batch 10] Loss: -129406314086.400\n",
      "[Epoch 57, Mini-batch 10] Loss: -137559562649.600\n",
      "[Epoch 58, Mini-batch 10] Loss: -146111229132.800\n",
      "[Epoch 59, Mini-batch 10] Loss: -154949464883.200\n",
      "[Epoch 60, Mini-batch 10] Loss: -164105638707.200\n",
      "[Epoch 61, Mini-batch 10] Loss: -173841521049.600\n",
      "[Epoch 62, Mini-batch 10] Loss: -183781081088.000\n",
      "[Epoch 63, Mini-batch 10] Loss: -194199378329.600\n",
      "[Epoch 64, Mini-batch 10] Loss: -205048029184.000\n",
      "[Epoch 65, Mini-batch 10] Loss: -216217039667.200\n",
      "[Epoch 66, Mini-batch 10] Loss: -227866768179.200\n",
      "[Epoch 67, Mini-batch 10] Loss: -239952979558.400\n",
      "[Epoch 68, Mini-batch 10] Loss: -252456570060.800\n",
      "[Epoch 69, Mini-batch 10] Loss: -265337266176.000\n",
      "[Epoch 70, Mini-batch 10] Loss: -278752172441.600\n",
      "[Epoch 71, Mini-batch 10] Loss: -292590177484.800\n",
      "[Epoch 72, Mini-batch 10] Loss: -307113517056.000\n",
      "[Epoch 73, Mini-batch 10] Loss: -321818030899.200\n",
      "[Epoch 74, Mini-batch 10] Loss: -337100845875.200\n",
      "[Epoch 75, Mini-batch 10] Loss: -353053052108.800\n",
      "[Epoch 76, Mini-batch 10] Loss: -369351757004.800\n",
      "[Epoch 77, Mini-batch 10] Loss: -386276173414.400\n",
      "[Epoch 78, Mini-batch 10] Loss: -403356558950.400\n",
      "[Epoch 79, Mini-batch 10] Loss: -421541720883.200\n",
      "[Epoch 80, Mini-batch 10] Loss: -439823240396.800\n",
      "[Epoch 81, Mini-batch 10] Loss: -458944197427.200\n",
      "[Epoch 82, Mini-batch 10] Loss: -478344350924.800\n",
      "[Epoch 83, Mini-batch 10] Loss: -498607803596.800\n",
      "[Epoch 84, Mini-batch 10] Loss: -519348971110.400\n",
      "[Epoch 85, Mini-batch 10] Loss: -540668116992.000\n",
      "[Epoch 86, Mini-batch 10] Loss: -562451821363.200\n",
      "[Epoch 87, Mini-batch 10] Loss: -584871824588.800\n",
      "[Epoch 88, Mini-batch 10] Loss: -608182337536.000\n",
      "[Epoch 89, Mini-batch 10] Loss: -631838690508.800\n",
      "[Epoch 90, Mini-batch 10] Loss: -656113519820.800\n",
      "[Epoch 91, Mini-batch 10] Loss: -681238875340.800\n",
      "[Epoch 92, Mini-batch 10] Loss: -706610344755.200\n",
      "[Epoch 93, Mini-batch 10] Loss: -732936005222.400\n",
      "[Epoch 94, Mini-batch 10] Loss: -760061303193.600\n",
      "[Epoch 95, Mini-batch 10] Loss: -787451189657.600\n",
      "[Epoch 96, Mini-batch 10] Loss: -816119919411.200\n",
      "[Epoch 97, Mini-batch 10] Loss: -844871702937.600\n",
      "[Epoch 98, Mini-batch 10] Loss: -874666544332.800\n",
      "[Epoch 99, Mini-batch 10] Loss: -905169534976.000\n",
      "[Epoch 100, Mini-batch 10] Loss: -936400361881.600\n",
      "Epoch 100/100, Validation Loss: -948898321749.3334, Accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        labels_one_hot = torch.nn.functional.one_hot(torch.argmax(labels, dim=1), num_classes=NUM_CLASSES).float()\n",
    "        loss = criterion(outputs, labels_one_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print(f'[Epoch {epoch + 1}, Mini-batch {i + 1}] Loss: {running_loss / 10:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            labels_one_hot = torch.nn.functional.one_hot(torch.argmax(labels, dim=1), num_classes=NUM_CLASSES).float()\n",
    "            loss = criterion(outputs, labels_one_hot)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == torch.argmax(labels_one_hot, dim=1)).sum().item()\n",
    "\n",
    "\n",
    "print(f'Epoch {epoch + 1}/{EPOCHS}, Validation Loss: {val_loss / len(val_loader):.4f}, Accuracy: {(correct / total) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_parquet_to_npy(input_folder, output_folder):\n",
    "#     npy_output_folder = os.path.join(output_folder, 'npy_data')\n",
    "    \n",
    "#     # Ensure the output directory exists\n",
    "#     os.makedirs(npy_output_folder, exist_ok=True)\n",
    "    \n",
    "#     for root, dirs, files in os.walk(input_folder):\n",
    "#         for file in files:\n",
    "#             if file.endswith('.parquet'):\n",
    "#                 parquet_path = os.path.join(root, file)\n",
    "#                 df = pd.read_parquet(parquet_path)\n",
    "#                 eeg_data = df.to_numpy()\n",
    "#                 relative_path = os.path.relpath(parquet_path, input_folder)\n",
    "                \n",
    "#                 # Create the corresponding directory structure in the npy_data folder\n",
    "#                 output_subfolder = os.path.join(npy_output_folder, os.path.dirname(relative_path))\n",
    "#                 os.makedirs(output_subfolder, exist_ok=True)\n",
    "#                 np.save(os.path.join(output_subfolder, file.replace('.parquet', '.npy')), eeg_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
