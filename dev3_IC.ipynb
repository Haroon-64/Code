{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from skimage.transform import resize\n",
    "from skimage.restoration import denoise_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASESS = 6\n",
    "NUM_FILES = 100\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224,3)  # len, width, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_parquet_to_npy(input_folder, output_folder):\n",
    "#     npy_output_folder = os.path.join(output_folder, 'npy_data')\n",
    "    \n",
    "#     # Ensure the output directory exists\n",
    "#     os.makedirs(npy_output_folder, exist_ok=True)\n",
    "    \n",
    "#     for root, dirs, files in os.walk(input_folder):\n",
    "#         for file in files:\n",
    "#             if file.endswith('.parquet'):\n",
    "#                 parquet_path = os.path.join(root, file)\n",
    "#                 df = pd.read_parquet(parquet_path)\n",
    "#                 eeg_data = df.to_numpy()\n",
    "#                 relative_path = os.path.relpath(parquet_path, input_folder)\n",
    "                \n",
    "#                 # Create the corresponding directory structure in the npy_data folder\n",
    "#                 output_subfolder = os.path.join(npy_output_folder, os.path.dirname(relative_path))\n",
    "#                 os.makedirs(output_subfolder, exist_ok=True)\n",
    "#                 np.save(os.path.join(output_subfolder, file.replace('.parquet', '.npy')), eeg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_folder, num_files=None):\n",
    "    \"\"\"\n",
    "    Read Spectrograms data from .npy files in the specified data folder.\n",
    "\n",
    "    Parameters:\n",
    "    - data_folder (str): Path to the main data folder containing 'train' and 'test' subfolders.\n",
    "    - num_files (int or None): Number of files to read from each subfolder. If None, all files will be read.\n",
    "\n",
    "    Returns:\n",
    "    - train (array[Tuple[np.ndarray, np.ndarray]]): List of tuples containing train EEG data.\n",
    "    - test (array[Tuple[np.ndarray, np.ndarray]]): List of tuples containing test EEG data.\n",
    "    - train_labels (pd.DataFrame): DataFrame containing train labels.\n",
    "    - test_labels (pd.DataFrame): DataFrame containing test labels.\n",
    "    \"\"\"\n",
    "    train_spec_folder = os.path.join(data_folder, 'train_spectrograms')\n",
    "    test_spec_folder = os.path.join(data_folder, 'test_spectrograms')\n",
    "\n",
    "    def read_npy_folder(folder_path, n_files=None):\n",
    "        arrays = []\n",
    "        files_to_read = os.listdir(folder_path)[:n_files] if n_files else os.listdir(folder_path)\n",
    "        for file in files_to_read:\n",
    "            if file.endswith('.npy'):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                array = np.load(file_path)\n",
    "                arrays.append(array)\n",
    "        print(f\"Read {len(arrays)} files from {folder_path}.\")\n",
    "        return arrays\n",
    "\n",
    "    # Read EEG data\n",
    "    train_spec = read_npy_folder(train_spec_folder, num_files)\n",
    "    test_spec = read_npy_folder(test_spec_folder)\n",
    "\n",
    "    train_labels = pd.read_csv(os.path.join(data_folder, 'train.csv'), nrows=num_files)\n",
    "    test_labels = pd.read_csv(os.path.join(data_folder, 'test.csv'))\n",
    "\n",
    "    return train_spec, test_spec, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spec(X_train_spec, img_size=(224, 224), n_channels=3):\n",
    "    \"\"\" Preprocess spectrograms:\n",
    "        1. Resize spectrograms\n",
    "        3. Denoise\n",
    "\n",
    "    Args:\n",
    "        X_train_spec : array-like, shape (n_samples, n_features)\n",
    "            Spectrograms data.\n",
    "        img_size : tuple, optional\n",
    "            Size to which spectrograms should be resized, defaults to (224, 224).\n",
    "        n_channels : int, optional\n",
    "            Number of color channels, defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        array-like, shape (n_samples, img_size[0], img_size[1], n_channels)\n",
    "            Preprocessed spectrograms.\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessed_specs = []\n",
    "    for spec in X_train_spec:\n",
    "        if spec.size == 0:\n",
    "            preprocessed_specs.append(np.zeros((*img_size, n_channels)))\n",
    "        else:\n",
    "            resized_spec = resize(spec, (*img_size, n_channels))\n",
    "            denoised_spec = denoise_wavelet(resized_spec)\n",
    "            preprocessed_specs.append(denoised_spec)\n",
    "    return np.array(preprocessed_specs, dtype=np.float32)\n",
    "\n",
    "def create_model(input_shape_spec, num_classes=6):\n",
    "    \"\"\"Create a model that can be trained for variable duration\n",
    "    spectrograms data.\n",
    "\n",
    "    Args:\n",
    "        input_shape_spec : shape of one Spectrogram sample\n",
    "        num_classes : 6 for seizure, lpd, gpd, lrda, grda, other\n",
    "\n",
    "    Returns:\n",
    "        keras model\n",
    "    \"\"\"\n",
    "    MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/5\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
    "        hub.KerasLayer(MODEL_URL),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 100 files from data/npy_data/npy_data/train_spectrograms.\n",
      "Read 1 files from data/npy_data/npy_data/test_spectrograms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haroon/miniconda3/envs/tf/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/haroon/miniconda3/envs/tf/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "train,_test,train_labels,_test_labels = read_data('data/npy_data/npy_data',num_files=NUM_FILES)\n",
    "data = preprocess_spec(train)\n",
    "labels = pd.read_csv('train.csv', nrows=NUM_FILES)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    data, train_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_train = label_encoder.fit_transform(y_train[:, 8])\n",
    "encoded_labels_val = label_encoder.fit_transform(y_val[:, 8])\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(encoded_labels_train, num_classes=NUM_CLASESS).astype('float32')\n",
    "y_val = tf.keras.utils.to_categorical(encoded_labels_val, num_classes=NUM_CLASESS).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(NUM_CLASESS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss = tf.keras.losses.KLDivergence(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 3s 54ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 53ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 48ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 48ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 45ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 45ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 45ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 45ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 45ms/step - loss: nan - accuracy: 0.0125\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 46ms/step - loss: nan - accuracy: 0.0125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f15d8747950>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
