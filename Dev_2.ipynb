{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T11:38:24.294177483Z",
     "start_time": "2024-01-30T11:38:24.291001806Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ProgressBarc' from 'dask.diagnostics' (/home/haroon/miniconda3/envs/tf/lib/python3.11/site-packages/dask/diagnostics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProgressBarc   \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ProgressBarc' from 'dask.diagnostics' (/home/haroon/miniconda3/envs/tf/lib/python3.11/site-packages/dask/diagnostics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "from tqdm import tqdm\n",
    "from dask.diagnostics import ProgressBarc   \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T11:38:59.815814157Z",
     "start_time": "2024-01-30T11:38:59.773451047Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_parquet_to_npy(file_path, save_path):\n",
    "    dataframe = pd.read_parquet(file_path)\n",
    "    data_array = dataframe.to_numpy()\n",
    "    np.save(save_path, data_array)\n",
    "\n",
    "def convert_folder_to_npy(folder_path, save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True) \n",
    "\n",
    "    files_to_convert = [file for file in os.listdir(folder_path) if file.endswith('.parquet')]\n",
    "    \n",
    "    for file in tqdm(files_to_convert, desc=f\"Converting files in {folder_path} to .npy\", unit=\"file\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        save_path = os.path.join(save_folder, f\"{os.path.splitext(file)[0]}.npy\")\n",
    "        convert_parquet_to_npy(file_path, save_path)\n",
    "\n",
    "def pad_or_truncate(data_point, target_shape):\n",
    "    current_shape = data_point.shape\n",
    "    if current_shape == target_shape:\n",
    "        return data_point\n",
    "    elif current_shape[0] < target_shape[0]:\n",
    "        # Pad the data point along the first axis\n",
    "        padding = [(0, target_shape[0] - current_shape[0])] + [(0, 0)] * (len(current_shape) - 1)\n",
    "        return np.pad(data_point, padding, mode='constant', constant_values=0)\n",
    "    else:\n",
    "        # Truncate the data point along the first axis\n",
    "        return data_point[:target_shape[0], ...]\n",
    "\n",
    "def chunked_concatenate(file_paths, chunk_size, target_shape):\n",
    "    chunks = []\n",
    "    for chunk_start in range(0, len(file_paths), chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, len(file_paths))\n",
    "        chunk_data = [np.load(file) for file in file_paths[chunk_start:chunk_end]]\n",
    "        chunks.append(chunk_data)\n",
    "    return chunks\n",
    "\n",
    "def flatten_chunks(chunks, target_shape):\n",
    "    flattened_data = [pad_or_truncate(data_point, target_shape) for chunk in chunks for data_point in chunk]\n",
    "    return flattened_data\n",
    "\n",
    "def load_data(path, num_files=None, chunk_size=100, target_shape = (85199 - 10000 + 1, 20)):\n",
    "    npy_train_save_folder = os.path.join(path, 'npy_train')\n",
    "    npy_test_save_folder = os.path.join(path, 'npy_test')\n",
    "\n",
    "    # Read Numpy files for train data in chunks\n",
    "    train_eeg_files = [os.path.join(npy_train_save_folder, file) for file in os.listdir(npy_train_save_folder)[:num_files] if file.endswith('.npy')]\n",
    "    train_eeg_chunks = chunked_concatenate(train_eeg_files, chunk_size, target_shape) if train_eeg_files else None\n",
    "\n",
    "    # Read Numpy files for test data in chunks\n",
    "    test_eeg_files = [os.path.join(npy_test_save_folder, file) for file in os.listdir(npy_test_save_folder) if file.endswith('.npy')]\n",
    "    test_eeg_chunks = chunked_concatenate(test_eeg_files, chunk_size, target_shape) if test_eeg_files else None\n",
    "\n",
    "    if train_eeg_chunks is not None:\n",
    "        # Interpolate NaN values for each chunk\n",
    "        for i, chunk in enumerate(train_eeg_chunks):\n",
    "            train_eeg_chunks[i] = [np.nan_to_num(data_point, nan=np.nanmean(data_point)) for data_point in chunk]\n",
    "            print(\"NaN values in train_eeg chunk {}: {}\".format(i, any(np.isnan(data_point).any() for data_point in train_eeg_chunks[i])))\n",
    "\n",
    "    # Read labels\n",
    "    train_labels = pd.read_csv('train.csv', nrows=num_files)\n",
    "    test_labels = pd.read_csv('test.csv')\n",
    "\n",
    "    # Flatten the chunks to get a list of data points\n",
    "    flattened_train_eeg = flatten_chunks(train_eeg_chunks, target_shape) if train_eeg_chunks else None\n",
    "    flattened_test_eeg = flatten_chunks(test_eeg_chunks, target_shape) if test_eeg_chunks else None\n",
    "\n",
    "    return flattened_train_eeg, flattened_test_eeg, train_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once\n",
    "# convert_folder_to_npy('data/train_eegs', 'data/npy_train')\n",
    "# convert_folder_to_npy('data/test_eegs', 'data/npy_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in train_eeg chunk 0: False\n",
      "NaN values in train_eeg chunk 1: False\n"
     ]
    }
   ],
   "source": [
    "flattened_train_eeg, flattened_test_eeg, train_labels, test_labels = load_data('data', num_files=100, chunk_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.array(flattened_train_eeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
